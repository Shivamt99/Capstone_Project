{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all packages & Dependencies\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from utils import *\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy import ndimage, io, misc\n",
    "from xml.dom import minidom\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\dog-breed-classifier\n"
     ]
    }
   ],
   "source": [
    "cd E:\\dog-breed-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Path to Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_create(path):\n",
    "    if  not os.path.exists(path):\n",
    "        os.mkdir(path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_create('train')\n",
    "file_create('test')\n",
    "file_create('cropped')\n",
    "file_create('cropped/train')\n",
    "file_create('cropped/test')\n",
    "\n",
    "# Data Source\n",
    "url = 'http://vision.stanford.edu/aditya86/ImageNetDogs/'\n",
    "last_percent_reported = None\n",
    "data_root = '.'\n",
    "num_classes = 120\n",
    "image_size = 224\n",
    "num_channels = 3\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extraction of data\n",
    "# Source: stackoverflow\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception('Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, check_classes=True, force=False):\n",
    "    root = os.path.splitext(filename)[0]  # remove .tar\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    if check_classes:\n",
    "        data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]\n",
    "        if len(data_folders) != num_classes:\n",
    "            raise Exception('Expected %d folders, one per class. Found %d instead.' % (num_classes, len(data_folders)))\n",
    "        print('Completed extraction of %s.' % filename)\n",
    "        return data_folders\n",
    "    else:\n",
    "        print('Completed extraction of %s.' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting & Downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: images.tar\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\images.tar\n",
      "Attempting to download: annotation.tar\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\annotation.tar\n",
      "Attempting to download: lists.tar\n",
      "0%..5%..10%..15%..20%..25%..30%..35%..40%..45%...................80%..85%..90%..95%..100%\n",
      "Download Complete!\n",
      "Found and verified .\\lists.tar\n"
     ]
    }
   ],
   "source": [
    "images_filename = maybe_download('images.tar', 793579520)\n",
    "annotation_filename = maybe_download('annotation.tar', 21852160)\n",
    "lists_filename = maybe_download('lists.tar', 481280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for images. This may take a while. Please wait.\n",
      "Completed extraction of images.tar.\n",
      "annotation already present - Skipping extraction of annotation.tar.\n",
      "Completed extraction of annotation.tar.\n",
      "Extracting data for .\\lists. This may take a while. Please wait.\n",
      "Completed extraction of .\\lists.tar.\n"
     ]
    }
   ],
   "source": [
    "images_filename = 'images.tar'\n",
    "annotation_filename = 'annotation.tar'\n",
    "\n",
    "images_folders = maybe_extract(images_filename)\n",
    "annotation_folders = maybe_extract(annotation_filename)\n",
    "maybe_extract(lists_filename, check_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cropped Train-Test Images\n",
    "\n",
    "for folder in images_folders:\n",
    "    os.makedirs(\"train/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"test/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"cropped/train/\"+folder.split(\"\\\\\")[-1])\n",
    "    os.makedirs(\"cropped/test/\"+folder.split(\"\\\\\")[-1])\n",
    "\n",
    "test_list = io.loadmat('test_list.mat')['file_list']\n",
    "train_list = io.loadmat('train_list.mat')['file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_data_files(image_list, new_folder):\n",
    "    for file in image_list:\n",
    "        if os.path.exists('Images/'+file[0][0]):\n",
    "            shutil.move('Images/'+file[0][0],new_folder+'/'+file[0][0])\n",
    "        elif not os.path.exists(new_folder+'/'+file[0][0]):\n",
    "           print('%s does not exist, it may be missing' % os.path.exists('./images/'+file[0][0]))\n",
    "    return [new_folder+'/'+d for d in sorted(os.listdir(new_folder)) if os.path.isdir(os.path.join(new_folder, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_folders = move_data_files(test_list, 'test')\n",
    "train_folders = move_data_files(train_list, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 229, 3)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data from Single Breed\n",
    "# Source: Stackoverflow\n",
    "\n",
    "def load_breed(folder):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size,num_channels), dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = folder+'/'+image\n",
    "        try:\n",
    "            \n",
    "            image_data = misc.imread(image_file)\n",
    "            \n",
    "            annon_file = 'Annotation' + '/' + folder.split('/')[-1] + '/' + image.split('.')[0]\n",
    "            annon_xml = minidom.parse(annon_file)\n",
    "            xmin = int(annon_xml.getElementsByTagName('xmin')[0].firstChild.nodeValue)\n",
    "            ymin = int(annon_xml.getElementsByTagName('ymin')[0].firstChild.nodeValue)\n",
    "            xmax = int(annon_xml.getElementsByTagName('xmax')[0].firstChild.nodeValue)\n",
    "            ymax = int(annon_xml.getElementsByTagName('ymax')[0].firstChild.nodeValue)\n",
    "            \n",
    "            new_image_data = image_data[ymin:ymax,xmin:xmax,:]\n",
    "            new_image_data = misc.imresize(new_image_data, (image_size, image_size))\n",
    "            misc.imsave('cropped/' + folder + '/' + image, new_image_data)\n",
    "            dataset[num_images, :, :, :] = new_image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :, :]\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    return dataset\n",
    "\n",
    "# Check the Img Shape\n",
    "\n",
    "from keras.preprocessing import image\n",
    "i=image.load_img('cropped/test/n02085620-Chihuahua/n02085620_588.jpg',target_size=(229,229))\n",
    "image.img_to_array(i).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/n02105855-Shetland_sheepdog\n",
      "Full dataset tensor: (100, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "def maybe_pickle(data_folders, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_breed(folder)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names\n",
    "\n",
    "# Convert to 4D Tesnor\n",
    "\n",
    "dataset = load_breed('train/n02105855-Shetland_sheepdog')\n",
    "with open('n02105855-Shetland_sheepdog.pickle', 'wb') as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train, Test- pickled Datasets\n",
    "\n",
    "train_folders=os.listdir('train')\n",
    "train_folders=['train'+'/'+d for d in train_folders]\n",
    "test_folders=os.listdir('test')\n",
    "test_folders=['test'+'/'+d for d in test_folders]\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, force=True)\n",
    "test_datasets = maybe_pickle(test_folders, force=True)\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows,img_size, img_size,num_channels), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all datasets\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0, even_size=True):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                breed_set = pickle.load(f)\n",
    "                np.random.shuffle(breed_set)\n",
    "                \n",
    "            if not even_size:\n",
    "                tsize_per_class,end_l = len(breed_set),len(breed_set)\n",
    "                end_t = start_t + tsize_per_class\n",
    "                \n",
    "            if valid_dataset is not None:\n",
    "                valid_breed = breed_set[:vsize_per_class, :, :, :]\n",
    "                valid_dataset[start_v:end_v, :, :, :] = valid_breed\n",
    "                valid_labels[start_v:end_v] = label\n",
    "                start_v += vsize_per_class\n",
    "                end_v += vsize_per_class\n",
    "\n",
    "            \n",
    "            train_breed = breed_set[vsize_per_class:end_l, :, :, :]\n",
    "            train_dataset[start_t:end_t, :, :, :] = train_breed\n",
    "            train_labels[start_t:end_t] = label\n",
    "            start_t += tsize_per_class\n",
    "            end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train, Test, Validation Size\n",
    "\n",
    "train_size = 9600\n",
    "valid_size = 2400\n",
    "test_size = 8580\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size, even_size=False)\n",
    "a=[len(os.listdir(\"train\"+'/'+d)) for d in os.listdir('train') if  not d.endswith('pickle')]\n",
    "\n",
    "# Save numpy arrays\n",
    "\n",
    "from utils import *\n",
    "np.save(open('train_dataset.npy','wb'), train_dataset)\n",
    "np.save(open('train_labels.npy','wb'), train_labels)\n",
    "np.save(open('valid_dataset.npy','wb'), valid_dataset)\n",
    "np.save(open('valid_labels.npy','wb'), valid_labels)\n",
    "\n",
    "np.save(open('test_dataset.npy','wb'), test_dataset)\n",
    "np.save(open('test_labels.npy','wb'), test_labels) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
